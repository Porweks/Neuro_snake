{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eccbce11",
      "metadata": {
        "id": "eccbce11"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import torch\n",
        "from numpy import unravel_index as unravel\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "import heapq\n",
        "import math\n",
        "import os\n",
        "from collections import deque\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0267406a",
      "metadata": {
        "id": "0267406a"
      },
      "outputs": [],
      "source": [
        "MAX_MEMORY = 100_000\n",
        "BATCH_SIZE = 1000\n",
        "LR = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30ad760d-ccb6-42b2-ae9f-cfb61245c126",
      "metadata": {
        "id": "30ad760d-ccb6-42b2-ae9f-cfb61245c126"
      },
      "outputs": [],
      "source": [
        "action_dict = {'a': torch.tensor([0., -1.]), 'd': torch.tensor([0., 1.]), 'w': torch.tensor([-1., 0.]), 's': torch.tensor([1., 0.])}\n",
        "\n",
        "def do(snake: torch.Tensor, action):\n",
        "    reward = 0\n",
        "    positions = snake.flatten().topk(2)[1]\n",
        "    [pos_cur, pos_prev] = [torch.Tensor(unravel(x, snake.shape)) for x in positions]\n",
        "    #print('direction', (pos_cur - pos_prev)) # Направление движения\n",
        "    pos_next = (pos_cur + action) % torch.Tensor([snake.shape]).squeeze(0)\n",
        "\n",
        "    pos_cur = pos_cur.int()\n",
        "    pos_next = pos_next.int()\n",
        "\n",
        "    # Проверка на столкновение\n",
        "    if (snake[tuple(pos_next)] > 1).any():\n",
        "        reward = -10\n",
        "        return reward,(snake[tuple(pos_cur)] - 2).item()  # Возвращаем счёт (длина змейки минус 2)\n",
        "\n",
        "    # Кушаем яблоко\n",
        "    if snake[tuple(pos_next)] == -1:\n",
        "        pos_food = (snake == 0).flatten().to(torch.float).multinomial(1)[0] # Генерируем позицию яблока\n",
        "        snake[unravel(pos_food, snake.shape)] = -1 # Добавляем яблоко в игру\n",
        "        reward = 10\n",
        "\n",
        "    else: # Двигаемся в пустую клетку\n",
        "        snake[snake > 0] -= 1  # Устанавливаем все значения в теле змеи равными 1\n",
        "\n",
        "    snake[tuple(pos_next)] = snake[tuple(pos_cur)] + 1 # перемещаем голову\n",
        "    return reward, (snake[tuple(pos_cur)] - 2).item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1bb55ee-66a2-42ee-8758-0018f9ffb796",
      "metadata": {
        "id": "a1bb55ee-66a2-42ee-8758-0018f9ffb796"
      },
      "outputs": [],
      "source": [
        "class Neuro_BigBoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1=nn.Conv2d(1, 32, kernel_size=(3,3), padding = 1)\n",
        "        self.conv2=nn.Conv2d(32, 64, kernel_size=(3,3), padding = 1)\n",
        "        # self.fl = nn.Flatten()\n",
        "        self.fc1=nn.Linear(64*32*32, 1024)\n",
        "        self.fc2=nn.Linear(1024, 256)\n",
        "        self.fc3=nn.Linear(256,3)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x= F.relu(self.conv2(x))\n",
        "        x = x.view(-1, 32*32*64)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.softmax(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "    def save(self, file_name='model.pth'):\n",
        "        model_folder_path = './model'\n",
        "        if not os.path.exists(model_folder_path):\n",
        "            os.makedirs(model_folder_path)\n",
        "\n",
        "        file_name = os.path.join(model_folder_path, file_name)\n",
        "        torch.save(self.state_dict(), file_name)\n",
        "\n",
        "class Neuro_NotSoBigBoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1=nn.Linear(12, 256)\n",
        "        self.fc2=nn.Linear(256,3)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def save(self, file_name='model.pth'):\n",
        "        model_folder_path = './model'\n",
        "        if not os.path.exists(model_folder_path):\n",
        "            os.makedirs(model_folder_path)\n",
        "\n",
        "        file_name = os.path.join(model_folder_path, file_name)\n",
        "        torch.save(self.state_dict(), file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccee6c1e",
      "metadata": {
        "id": "ccee6c1e"
      },
      "outputs": [],
      "source": [
        "class QTrainer:\n",
        "    def __init__(self, model, lr, gamma):\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.model = model\n",
        "        self.model.load_state_dict(torch.load('model/model134.pth'))\n",
        "        self.optimizer = torch.optim.Adam(model.parameters(), lr=self.lr)\n",
        "        self.criterion = nn.MSELoss()##.cuda()\n",
        "\n",
        "    def train_step(self, state, action, reward, next_state, done):\n",
        "        # print(state)\n",
        "        state = torch.tensor(state, dtype=torch.float)\n",
        "        next_state = torch.tensor(next_state, dtype=torch.float)\n",
        "        action = torch.tensor(action, dtype=torch.long)\n",
        "        reward = torch.tensor(reward, dtype=torch.float)\n",
        "        state = state#.cuda()\n",
        "        next_state=next_state#.cuda()\n",
        "        action =action#.cuda()\n",
        "        reward#.cuda()\n",
        "        # (n, x)\n",
        "        # if len(state.shape) == 2:\n",
        "        #     # (1, x)\n",
        "        #     state = torch.unsqueeze(state, 0)#.cuda()\n",
        "        #     next_state = torch.unsqueeze(next_state, 0)#.cuda()\n",
        "        #     action = torch.unsqueeze(action, 0)#.cuda()\n",
        "        #     reward = torch.unsqueeze(reward, 0)#.cuda()\n",
        "        #     done = (done, )\n",
        "\n",
        "        # 1: predicted Q values with current state\n",
        "        # print(state.shape)\n",
        "        pred = self.model(state.unsqueeze(0))\n",
        "\n",
        "        target = pred.clone()\n",
        "        \n",
        "        Q_new = reward\n",
        "        if not done:\n",
        "            Q_new = reward + self.gamma * torch.max(self.model(next_state.unsqueeze(0)))\n",
        "        # print(target.shape)\n",
        "        target[0][torch.argmax(action).item()] = Q_new\n",
        "\n",
        "        # 2: Q_new = r + y * max(next_predicted Q value) -> only do this if not done\n",
        "        # pred.clone()\n",
        "        # preds[argmax(action)] = Q_new\n",
        "        self.optimizer.zero_grad()\n",
        "        loss = self.criterion(pred, target)\n",
        "        loss.backward()\n",
        "        # print(loss)\n",
        "\n",
        "        self.optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9e56e07-1c39-4668-9db9-70d2f6f671c0",
      "metadata": {
        "id": "f9e56e07-1c39-4668-9db9-70d2f6f671c0"
      },
      "outputs": [],
      "source": [
        "class Champion():\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.n_games = 0\n",
        "        self.gamma = 0.9 # discount rate\n",
        "        self.memory = deque(maxlen=MAX_MEMORY)\n",
        "        self.model = model\n",
        "        self.eps = 80\n",
        "        self.trainer = QTrainer(self.model, lr=LR, gamma=self.gamma)\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done)) # popleft if MAX_MEMORY is reached\n",
        "\n",
        "    def train_long_memory(self):\n",
        "        if len(self.memory) > BATCH_SIZE:\n",
        "            mini_sample = random.sample(self.memory, BATCH_SIZE) # list of tuples\n",
        "        else:\n",
        "            mini_sample = self.memory\n",
        "\n",
        "        for state, action, reward, next_state, done in mini_sample:\n",
        "           self.trainer.train_step(state, action, reward, next_state, done)\n",
        "\n",
        "    def train_short_memory(self, state, action, reward, next_state, done):\n",
        "        self.trainer.train_step(state, action, reward, next_state, done)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        # random moves: tradeoff exploration / exploitation\n",
        "        self.epsilon = (80-self.eps) - self.n_games\n",
        "        final_move = [0,0,0]\n",
        "        if random.randint(0, 200) < self.epsilon:\n",
        "            move = random.randint(0, 2)\n",
        "            final_move[move] = 1\n",
        "        else:\n",
        "            state = torch.tensor(state, dtype=torch.float)\n",
        "            # print(state.shape)\n",
        "            with torch.no_grad():\n",
        "                prediction = self.model(state)\n",
        "            move = torch.argmax(prediction).item()\n",
        "            final_move[move] = 1\n",
        "\n",
        "        return final_move\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4ad2e85",
      "metadata": {
        "id": "a4ad2e85"
      },
      "outputs": [],
      "source": [
        "class Node:\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x  # Координата x узла на карте\n",
        "        self.y = y  # Координата y узла на карте\n",
        "        self.g = 0  # Расстояние от начального узла до текущего узла\n",
        "        self.h = 0  # Примерное расстояние от текущего узла до конечного узла\n",
        "        self.f = 0  # Сумма g и h\n",
        "        self.parent = None  # Родительский узел, используется для восстановления пути\n",
        "\n",
        "    # Переопределяем оператор сравнения для сравнения узлов\n",
        "    def __lt__(self, other):\n",
        "        return self.f < other.f\n",
        "\n",
        "    # Переопределяем оператор равенства для сравнения узлов\n",
        "    def __eq__(self, other):\n",
        "        return self.x == other.x and self.y == other.y\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash((self.x, self.y))\n",
        "\n",
        "# Определяем функцию для нахождения пути с помощью алгоритма A*\n",
        "def astar(start, end, obstacles):\n",
        "\n",
        "    # Создаем начальный и конечный узлы\n",
        "    start_node = Node(start[0], start[1])\n",
        "    end_node = Node(end[0], end[1])\n",
        "\n",
        "    # Инициализируем очередь с приоритетами\n",
        "    open_list = []\n",
        "    heapq.heappush(open_list, start_node)\n",
        "\n",
        "    # Инициализируем множество посещенных узлов\n",
        "    closed_set = set()\n",
        "\n",
        "    # Пока очередь с приоритетами не пуста\n",
        "    while open_list:\n",
        "        # Извлекаем узел с наименьшей оценкой f\n",
        "        current_node = heapq.heappop(open_list)\n",
        "        # Если текущий узел является конечным\n",
        "        if current_node == end_node:\n",
        "            # Восстанавливаем путь от конечного узла до начального\n",
        "            path = []\n",
        "            while current_node.parent is not None:\n",
        "                path.append((current_node.x, current_node.y))\n",
        "                current_node = current_node.parent\n",
        "            return path[-1]\n",
        "\n",
        "        # Добавляем текущий узел в множество посещенных узлов\n",
        "        closed_set.add(current_node)\n",
        "\n",
        "        # Получаем соседние узлы\n",
        "        neighbors = []\n",
        "        for dx in range(-1, 2):\n",
        "            for dy in range(-1, 2):\n",
        "                # Игнорируем текущий узел\n",
        "                if not ((dx == -1 and dy == 0) or (dx == 0 and dy == 1) or (dx == 1 and dy == 0) or (dx == 0 and dy == -1)):\n",
        "                    continue\n",
        "                # Вычисляем координаты соседнего узла\n",
        "                x = current_node.x + dx\n",
        "                y = current_node.y + dy\n",
        "                # Игнорируем узлы за пределами карты\n",
        "                if x < 0 :\n",
        "                    x=len(obstacles)-1\n",
        "                if x>=len(obstacles):\n",
        "                    x=0\n",
        "                if y<0:\n",
        "                    y=len(obstacles)-1\n",
        "                if y>=len(obstacles):\n",
        "                    y=0\n",
        "                # Игнорируем препятствия\n",
        "                if obstacles[x][y] == 1:\n",
        "                    continue\n",
        "                # Создаем новый узел и добавляем его в список соседей\n",
        "                neighbor = Node(x, y)\n",
        "                neighbors.append(neighbor)\n",
        "\n",
        "        # Для каждого соседнего узла\n",
        "        for neighbor in neighbors:\n",
        "            # Если соседний узел уже был посещен, пропускаем его\n",
        "            if neighbor in closed_set:\n",
        "                continue\n",
        "\n",
        "            # Вычисляем расстояние от начального узла до соседнего узла\n",
        "            new_g = current_node.g + 1\n",
        "\n",
        "            # Если соседний узел уже находится в очереди с приоритетами\n",
        "            if nfo := next((n for n in open_list if n == neighbor), None):\n",
        "                # Если новое расстояние до соседнего узла меньше, чем старое, обновляем значения g, h и f\n",
        "                if new_g < nfo.g:\n",
        "                    nfo.g = new_g\n",
        "                    nfo.h = math.sqrt((end_node.x - nfo.x) ** 2 + (end_node.y - nfo.y) ** 2)\n",
        "                    nfo.f = nfo.g + nfo.h\n",
        "                    nfo.parent = current_node\n",
        "                    # Обновляем приоритет соседнего узла в очереди с приоритетами\n",
        "                    heapq.heapify(open_list)\n",
        "            else:\n",
        "                # Иначе добавляем соседний узел в очередь с приоритетами и вычисляем значения g, h и f\n",
        "                neighbor.g = new_g\n",
        "                neighbor.h = math.sqrt((end_node.x - neighbor.x) ** 2 + (end_node.y - neighbor.y) ** 2)\n",
        "                neighbor.f = neighbor.g + neighbor.h\n",
        "                neighbor.parent = current_node\n",
        "                heapq.heappush(open_list, neighbor)\n",
        "\n",
        "    # Если конечный узел недостижим, возвращаем None\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "445a9be8",
      "metadata": {
        "id": "445a9be8"
      },
      "outputs": [],
      "source": [
        "class Snake:\n",
        "    def __init__(self):\n",
        "        self.field = torch.zeros((32, 32), dtype=torch.float)\n",
        "        # self.field[0, :32] = torch.Tensor([1, 2, 3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32])\n",
        "        # self.field[1,:32] = torch.Tensor([64,63,62,61,60,59,58,57,56,55,54,53,52,51,50,49,48,47,46,45,44,43,42,41,40,39,38,37,36,35,34,33]) # [хвост, голова, яблоко]\n",
        "        self.field[0, :4] = torch.Tensor([1, 2, 3,4]) # [хвост, голова, яблоко]\n",
        "        self.field[1,5] =  torch.Tensor([-1])\n",
        "        # self.field[2,1] = torch.Tensor([65])\n",
        "        # self.field[2,5] =  torch.Tensor([-1])\n",
        "        self.a = torch.zeros((32, 32), dtype=torch.float)\n",
        "        self.dirrection = torch.tensor([0,1])\n",
        "        self.head_cords = [0,1]\n",
        "        self.apple_cords = [0,2]\n",
        "        self.neighbours = [[31,1],[0,0],[0,2],[1,1]]\n",
        "        self.collision = [False, True, False, False]\n",
        "        self.availible_passes = [torch.tensor([-1,0]),torch.tensor(self.dirrection),torch.tensor([1,0])]\n",
        "        self.old_distance = 0\n",
        "        self.new_distance = 0\n",
        "\n",
        "    def set_dirrection(self, dir):\n",
        "        dir = torch.tensor(dir)\n",
        "        if not torch.allclose(dir, self.dirrection):\n",
        "            self.dirrection = dir\n",
        "            if torch.allclose(dir, torch.tensor([0, 1])):\n",
        "                self.availible_passes = [torch.tensor([-1, 0]), torch.tensor(self.dirrection), torch.tensor([1, 0])]\n",
        "            if torch.allclose(dir, torch.tensor([1, 0])):\n",
        "                self.availible_passes = [torch.tensor([0, 1]), torch.tensor(self.dirrection), torch.tensor([0, -1])]\n",
        "            if torch.allclose(dir, torch.tensor([0, -1])):\n",
        "                self.availible_passes = [torch.tensor([1, 0]), torch.tensor(self.dirrection), torch.tensor([-1, 0])]\n",
        "            if torch.allclose(dir, torch.tensor([-1, 0])):\n",
        "                self.availible_passes = [torch.tensor([0, -1]), torch.tensor(self.dirrection), torch.tensor([0, 1])]\n",
        "\n",
        "    def set_head_cords(self,head):\n",
        "        self.head_cords = head\n",
        "\n",
        "    def set_apple_cords(self,apple):\n",
        "        self.apple_cords = apple\n",
        "\n",
        "    def set_neighbours(self):\n",
        "        counter = 0\n",
        "        for dx in range(-1, 2):\n",
        "            for dy in range(-1, 2):\n",
        "                # Игнорируем текущий узел\n",
        "                if not ((dx == -1 and dy == 0) or (dx == 0 and dy == 1) or (dx == 1 and dy == 0) or (dx == 0 and dy == -1)):\n",
        "                    continue\n",
        "                x = self.head_cords[0] + dx\n",
        "                y = self.head_cords[1] + dy\n",
        "\n",
        "                if x < 0 :\n",
        "                    x=31\n",
        "                if x>=32:\n",
        "                    x=0\n",
        "                if y<0:\n",
        "                    y=31\n",
        "                if y>=32:\n",
        "                    y=0\n",
        "\n",
        "                self.neighbours[counter] = [x,y]\n",
        "                if self.field[x][y] > 0:\n",
        "                    self.collision[counter] = True\n",
        "                else:\n",
        "                    self.collision[counter] = False\n",
        "                counter+=1\n",
        "\n",
        "    def get_state(self):\n",
        "        u=self.dirrection.tolist()==[-1,0]\n",
        "        l = self.dirrection.tolist()==[0,-1]\n",
        "        r =self.dirrection.tolist()==[0,1]\n",
        "        d =self.dirrection.tolist()==[1,0]\n",
        "        return [u,l,r,d,\n",
        "                self.head_cords[0]<self.apple_cords[0],self.head_cords[0]>self.apple_cords[0],\n",
        "                self.head_cords[1]<self.apple_cords[1],self.head_cords[1]>self.apple_cords[1],\n",
        "                (u==self.collision[0]==1 or l==self.collision[1]==1 or r==self.collision[2]==1 or d==self.collision[3]==1),\n",
        "                (u==self.collision[1]==1 or l==self.collision[3]==1 or r==self.collision[0]==1 or d==self.collision[2]==1),\n",
        "                (u==self.collision[2]==1 or l==self.collision[0]==1 or r==self.collision[3]==1 or d==self.collision[1]==1)]\n",
        "\n",
        "    def make_step(self, step):\n",
        "        step=self.availible_passes[np.argmax(step)]\n",
        "        reward = 0\n",
        "        reward, score = do(self.field, step)#, steps_before_eaten_apple)\n",
        "        a = torch.zeros(self.field.shape)\n",
        "        a[self.field>0]=1\n",
        "        a[self.field==self.field.max()]=2\n",
        "        a[self.field<0]=-1\n",
        "        head = [self.field.topk(1)[0].argmax().numpy(),self.field.topk(1)[1][self.field.topk(1)[0].argmax()].numpy()]\n",
        "        apple = [(self.field.argmin()/32).int().item(),\n",
        "            (self.field.argmin()%32).item()]\n",
        "        head[1]=head[1][0]\n",
        "        head_cords = [head[0].item(),head[1]]\n",
        "        self.set_apple_cords(apple)\n",
        "        # print(self,head_cords)\n",
        "        self.set_head_cords(head_cords)\n",
        "        # print(self.head_cords)\n",
        "        self.set_neighbours()\n",
        "        self.set_dirrection(step)\n",
        "        a_np = a.numpy()\n",
        "        a[self.field<0]=10\n",
        "        cv2.namedWindow('image', cv2.WINDOW_NORMAL)\n",
        "        cv2.imshow('image', a_np)\n",
        "        cv2.waitKey(1)\n",
        "        # cv2.destroyAllWindows()\n",
        "        done = False\n",
        "        if reward == -10:\n",
        "            done = True\n",
        "        return reward, done, score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15d11673",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15d11673",
        "outputId": "1b8694bd-c077-4fdd-da66-2d26b0fb7e2e"
      },
      "outputs": [],
      "source": [
        "\n",
        "plot_scores = []\n",
        "plot_mean_scores = []\n",
        "total_score = 0\n",
        "record = 0\n",
        "model = Neuro_NotSoBigBoss()#.cuda()\n",
        "agent = Champion(model)\n",
        "game = Snake()\n",
        "reward_per_long_moves = 0\n",
        "reward0_in_a_row=0\n",
        "new_distance = 0\n",
        "apple_eaten = 0\n",
        "old_apple_cords = []\n",
        "new_apple_cords = []\n",
        "old_dirrection = torch.tensor([])\n",
        "new_dirrection = torch.tensor([])\n",
        "bad = 0\n",
        "while True:\n",
        "    # get old state\n",
        "    state_old = game.get_state()\n",
        "    state_old.append(bad)\n",
        "    old_apple_cords = game.apple_cords\n",
        "    old_dirrection = game.dirrection\n",
        "    # get move\n",
        "    final_move = agent.get_action(state_old)\n",
        "    # perform move and get new state\n",
        "    game.old_distance = abs(game.head_cords[0]-game.apple_cords[0])+abs(game.head_cords[1]-game.apple_cords[1])\n",
        "    reward, done, score = game.make_step(final_move)\n",
        "    new_apple_cords = game.apple_cords\n",
        "    game.new_distance = abs(game.head_cords[0]-game.apple_cords[0])+abs(game.head_cords[1]-game.apple_cords[1])\n",
        "    new_dirrection = game.dirrection\n",
        "    if new_dirrection.tolist()==old_dirrection.tolist() and reward == 0:\n",
        "        bad+=1\n",
        "        if reward == 0 and bad>32:\n",
        "            reward = -bad/32\n",
        "    else:\n",
        "        bad = 0\n",
        "    if bad>65:\n",
        "        reward = -10\n",
        "        bad = 0\n",
        "        # print(new_dirrection.tolist(), old_dirrection.tolist())\n",
        "    if not new_apple_cords == old_apple_cords:\n",
        "        game.new_distance=0\n",
        "    if new_distance>31:\n",
        "        new_distance = 62 - new_distance\n",
        "    if reward == 10:\n",
        "        apple_eaten+=1\n",
        "        reward=13\n",
        "        bad = 0\n",
        "    #     if reward0_in_a_row<70:\n",
        "    #         reward+=5\n",
        "    #     else:\n",
        "    #         reward-=5*apple_eaten\n",
        "        reward0_in_a_row=0\n",
        "\n",
        "    if reward == -bad/32 or reward==0:\n",
        "        # reward = -0.25\n",
        "    #     if reward == 0:\n",
        "        reward += abs(32-game.new_distance)/8\n",
        "        if game.new_distance>game.old_distance:\n",
        "            reward = -reward\n",
        "    # if reward == -0.25:\n",
        "        reward0_in_a_row+=1\n",
        "        if reward0_in_a_row > 65:\n",
        "            reward -= reward0_in_a_row/100\n",
        "        if reward0_in_a_row>1400:\n",
        "            reward = -10\n",
        "    # print(reward)\n",
        "    if reward == -10:\n",
        "        apple_eaten=0\n",
        "        reward == -20\n",
        "        if done == True:\n",
        "            reward = -40 - score*5 \n",
        "            if score > record:\n",
        "                reward = -10 \n",
        "        else:\n",
        "            reward = -20 - score*5\n",
        "        done = True\n",
        "    #     reward = -5\n",
        "        # if score < record:\n",
        "        #     reward =-(score*record)*10\n",
        "        #     if reward < -100:\n",
        "        #         reward = -100\n",
        "        reward0_in_a_row=0\n",
        "\n",
        "    state_new = game.get_state()\n",
        "    state_new.append(bad>33)\n",
        "\n",
        "    # train short memory\n",
        "    agent.train_short_memory(state_old, final_move, reward, state_new, done)\n",
        "\n",
        "    # remember\n",
        "    agent.remember(state_old, final_move, reward, state_new, done)\n",
        "\n",
        "    if done:\n",
        "        print(game.collision)\n",
        "        # train long memory, plot result\n",
        "        game=Snake()\n",
        "        agent.n_games += 1\n",
        "        agent.train_long_memory()\n",
        "\n",
        "        if score > record:\n",
        "            record = score\n",
        "            agent.model.save()\n",
        "\n",
        "\n",
        "        print('Game', agent.n_games, 'Score', score, 'Record:', record)\n",
        "\n",
        "        plot_scores.append(score)\n",
        "        total_score += score\n",
        "        mean_score = total_score / agent.n_games\n",
        "        plot_mean_scores.append(mean_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e733d47b",
      "metadata": {
        "id": "e733d47b",
        "outputId": "c3aea8cd-ecb2-4918-9561-520d5d76d9ca"
      },
      "outputs": [],
      "source": [
        "\n",
        "snake = Snake()\n",
        "reward = 0\n",
        "while True:\n",
        "    a = torch.zeros(snake.field.shape)\n",
        "    a[snake.field>0]=1\n",
        "    a[snake.field==snake.field.max()]=2\n",
        "    a[snake.field<0]=-1\n",
        "    head = [snake.field.topk(1)[0].argmax().numpy(),snake.field.topk(1)[1][snake.field.topk(1)[0].argmax()].numpy()]\n",
        "    apple = [(snake.field.argmin()/32).int().item(),\n",
        "        (snake.field.argmin()%32).item()]\n",
        "    head[1]=head[1][0]\n",
        "    head_cords = [head[0].item(),head[1]]\n",
        "    snake.set_apple_cords(apple)\n",
        "    snake.set_head_cords(head_cords)\n",
        "    snake.set_neighbours()\n",
        "    if reward == -100:\n",
        "        break\n",
        "    path = astar( head_cords, apple, a)\n",
        "    if path == None:\n",
        "        break\n",
        "    cords = np.array(path)\n",
        "    # print(cords, np.array(head_cords))\n",
        "    cords-=np.array(head_cords)\n",
        "    if cords[0]==-31:\n",
        "        cords[0]=1\n",
        "    if cords[0]==31:\n",
        "        cords[0]=-1\n",
        "    if cords[1]==-31:\n",
        "        cords[1]=1\n",
        "    if cords[1]==31:\n",
        "        cords[1]=-1\n",
        "    # fig, ax = plt.subplots(1, 1)\n",
        "    # img = ax.imshow(a)\n",
        "    action = {'val': 1}\n",
        "    n = 0\n",
        "    score = None\n",
        "    # img.set_data(a)\n",
        "    cords_list=cords.tolist()\n",
        "    snake.set_dirrection(cords_list)\n",
        "    reward, score = do(snake.field, cords)\n",
        "    n += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b41baea",
      "metadata": {
        "id": "7b41baea",
        "outputId": "7c7a4eff-4b03-4b86-8c52-29680589687e"
      },
      "outputs": [],
      "source": [
        "snake=Snake()\n",
        "a = torch.zeros(snake.field.shape)\n",
        "a[snake.field>0]=1\n",
        "a[snake.field==snake.field.max()]=2\n",
        "a[snake.field<0]=-1\n",
        "head = [snake.field.topk(1)[0].argmax().numpy(),snake.field.topk(1)[1][snake.field.topk(1)[0].argmax()].numpy()]\n",
        "apple = [(snake.field.argmin()/32).int().item(),\n",
        "    (snake.field.argmin()%32).item()]\n",
        "head[1]=head[1][0]\n",
        "head_cords = [head[0].item(),head[1]]\n",
        "snake.set_apple_cords(apple)\n",
        "snake.set_head_cords(head_cords)\n",
        "snake.set_neighbours()\n",
        "snake.set_dirrection([-1,0])\n",
        "fig, ax = plt.subplots(1, 1)\n",
        "a = torch.zeros(snake.field.shape)\n",
        "positions = snake.field.flatten().topk(2)[1]\n",
        "[pos_cur, pos_prev] = [torch.Tensor(unravel(x, snake.field.shape)) for x in positions]\n",
        "a[snake.field>0]=1\n",
        "a[snake.field==snake.field.max()]=2\n",
        "a[snake.field<0]=-1\n",
        "# print(a)\n",
        "img = ax.imshow(a)\n",
        "action = {'val': 1}\n",
        "# print(a)\n",
        "n = 0\n",
        "score = None\n",
        "\n",
        "print(snake.collision)\n",
        "while n<1:\n",
        "    img.set_data(a)\n",
        "    # reward, score = do(snake.field, action_dict['a'])\n",
        "    n += 1\n",
        "# print(reward, score)\n",
        "# print(score)\n",
        "state = snake.get_state()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d3b2be0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d3b2be0",
        "outputId": "aec75ab5-8dca-4ef7-966d-e19bf1ae79ab"
      },
      "outputs": [],
      "source": [
        "\n",
        "plot_scores = []\n",
        "plot_mean_scores = []\n",
        "total_score = 0\n",
        "record = 0\n",
        "model = Neuro_NotSoBigBoss()#.cuda()\n",
        "agent = Champion(model)\n",
        "game = Snake()\n",
        "reward_per_long_moves = 0\n",
        "reward0_in_a_row=0\n",
        "new_distance = 0\n",
        "apple_eaten = 0\n",
        "old_apple_cords = []\n",
        "new_apple_cords = []\n",
        "old_dirrection = torch.tensor([])\n",
        "new_dirrection = torch.tensor([])\n",
        "bad = 0\n",
        "while True:\n",
        "    # get old state\n",
        "    state_old = game.get_state()\n",
        "    state_old.append(bad>33)\n",
        "    old_apple_cords = game.apple_cords\n",
        "    old_dirrection = game.dirrection\n",
        "    # get move\n",
        "    final_move = agent.get_action(state_old)\n",
        "    # perform move and get new state\n",
        "    game.old_distance = abs(game.head_cords[0]-game.apple_cords[0])+abs(game.head_cords[1]-game.apple_cords[1])\n",
        "    reward, done, score = game.make_step(final_move)\n",
        "    new_apple_cords = game.apple_cords\n",
        "    game.new_distance = abs(game.head_cords[0]-game.apple_cords[0])+abs(game.head_cords[1]-game.apple_cords[1])\n",
        "    new_dirrection = game.dirrection\n",
        "    if new_dirrection.tolist()==old_dirrection.tolist() and reward == 0:\n",
        "        bad+=1\n",
        "        if reward == 0 and bad>32:\n",
        "            reward = -bad/32\n",
        "    else:\n",
        "        bad = 0\n",
        "    if bad>65:\n",
        "        reward = -10\n",
        "        bad = 0\n",
        "        # print(new_dirrection.tolist(), old_dirrection.tolist())\n",
        "    if not new_apple_cords == old_apple_cords:\n",
        "        game.new_distance=0\n",
        "    if new_distance>31:\n",
        "        new_distance = 62 - new_distance\n",
        "    if reward == 10:\n",
        "        apple_eaten+=1\n",
        "        reward=13\n",
        "        bad = 0\n",
        "    #     if reward0_in_a_row<70:\n",
        "    #         reward+=5\n",
        "    #     else:\n",
        "    #         reward-=5*apple_eaten\n",
        "        reward0_in_a_row=0\n",
        "\n",
        "    if reward == -bad/32 or reward==0:\n",
        "        # reward = -0.25\n",
        "    #     if reward == 0:\n",
        "        if game.new_distance>game.old_distance:\n",
        "            reward = -reward\n",
        "    # if reward == -0.25:\n",
        "        reward0_in_a_row+=1\n",
        "        if reward0_in_a_row > 65:\n",
        "            reward -= reward0_in_a_row/100\n",
        "        if reward0_in_a_row>1400:\n",
        "            reward = -10\n",
        "    # print(reward)\n",
        "    if reward == -10:\n",
        "        apple_eaten=0\n",
        "        reward == -20\n",
        "        if done == True:\n",
        "            reward = -40 - (record-score)*7.5 \n",
        "            if score > record:\n",
        "                reward = -10 \n",
        "        else:\n",
        "            reward = -20 - (record-score)*7.5\n",
        "        done = True\n",
        "    #     reward = -5\n",
        "        # if score < record:\n",
        "        #     reward =-(score*record)*10\n",
        "        #     if reward < -100:\n",
        "        #         reward = -100\n",
        "        reward0_in_a_row=0\n",
        "\n",
        "    state_new = game.get_state()\n",
        "    state_new.append(bad>33)\n",
        "    # if bad>33:\n",
        "    #     print(bad)\n",
        "\n",
        "    # train short memory\n",
        "\n",
        "    if done:\n",
        "        # train long memory, plot result\n",
        "        if record< score:\n",
        "            record = score\n",
        "        # game=Snake()\n",
        "        agent.n_games += 1\n",
        "        game = Snake()\n",
        "\n",
        "        print('Game', agent.n_games, 'Score', score, 'Record:', record)#, 'Old state', state_old, 'New state', state_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10c3f0fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "game.make_step([0,0,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f184209",
      "metadata": {},
      "outputs": [],
      "source": [
        "game.dirrection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f725c49",
      "metadata": {},
      "outputs": [],
      "source": [
        "game.collision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75754443",
      "metadata": {},
      "outputs": [],
      "source": [
        "game.get_state()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "897099f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "game.head_cords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5448aea",
      "metadata": {},
      "outputs": [],
      "source": [
        "a = [[i for i in range(32,64)]]\n",
        "len(a[0][::-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5fde6be",
      "metadata": {},
      "outputs": [],
      "source": [
        "field = torch.zeros((32, 32), dtype=torch.float)\n",
        "field[0, :32] = torch.Tensor([i for i in range(1,33)])\n",
        "print(field[0])\n",
        "for i in range (32,321):\n",
        "    field[int(i/32)][i%32] = i+1\n",
        "    # if field[int(i/32)][i%32-1] == 0:\n",
        "    #     field[int(i/32)][i%32-1] = int(i/32)*32\n",
        "# field[1][0] = 33\n",
        "for i in range (1,11):\n",
        "    field[i] = field[i].flip(0)\n",
        "    print(field[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4b88dc2",
      "metadata": {},
      "outputs": [],
      "source": [
        "field[0][::-1]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
